{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9ad98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.2.0\n",
    "!pip install torch==1.5.1 torchvision==0.6.1\n",
    "!pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "497973d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Launch SageMaker training job from local.\"\"\"\n",
    "from datetime import datetime\n",
    "from sagemaker.pytorch.estimator import PyTorch\n",
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b68cc4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e9cac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define AWS sessions.\n",
    "sagemaker_session = sagemaker.Session()\n",
    "# Define role arn with SageMaker and S3 access.\n",
    "role = f\"arn:aws:iam::046704982951:role/service-role/AmazonSageMaker-ExecutionRole-20210622T142702\"\n",
    "# Define S3 variables for data and model storage.\n",
    "bucket = \"sagemaker-dan-rasmussen\"\n",
    "model_prefix = \"sagemaker/amazon_review_classifier/train\"\n",
    "train_file = \"small_book_reviews.json\"\n",
    "input_path = f\"s3://{bucket}/{model_prefix}/input_data/{train_file}\"\n",
    "output_path = f\"s3://{bucket}/{model_prefix}/model\"\n",
    "code_path = f\"s3://{bucket}/{model_prefix}/src\"\n",
    "# Upload data to S3 where it can be accessed by SageMaker.\n",
    "sagemaker_session.upload_data(\n",
    "    path=f\"./data/{train_file}\",\n",
    "    bucket=bucket,\n",
    "    key_prefix=f\"{model_prefix}/input_data\"\n",
    ")\n",
    "# Define hyperparameters which get passed as command-line args to model.py.\n",
    "hyperparameters = {\n",
    "    \"input_path\": input_path, # Where our model will read the training data.\n",
    "    \"model_name\": \"distilbert-base-uncased\", # https://huggingface.co/distilbert-base-uncased\n",
    "    \"train_batch_size\": 32,\n",
    "    \"valid_batch_size\": 128,\n",
    "    \"epochs\": 2,\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"weight_decay\": .01,\n",
    "    \"max_sequence_length\": 128,\n",
    "    'max_data_rows': 1000\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d804379",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-24 20:29:07 Starting - Starting the training job...\n",
      "2021-06-24 20:29:09 Starting - Launching requested ML instancesProfilerReport-1624566546: InProgress\n",
      "......\n",
      "2021-06-24 20:30:36 Starting - Preparing the instances for training.........\n",
      "2021-06-24 20:31:59 Downloading - Downloading input data\n",
      "2021-06-24 20:31:59 Training - Downloading the training image........\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-06-24 20:33:17,000 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-06-24 20:33:17,033 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-06-24 20:33:23,256 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-06-24 20:33:23,530 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting numpy==1.19.5\n",
      "  Downloading numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\u001b[0m\n",
      "\u001b[34mCollecting pandas==1.1.5\n",
      "  Downloading pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.2.0\n",
      "  Downloading transformers-4.2.0-py3-none-any.whl (1.8 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas==1.1.5->-r requirements.txt (line 2)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas==1.1.5->-r requirements.txt (line 2)) (2021.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2021.4.4-cp36-cp36m-manylinux2014_x86_64.whl (722 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.2.0->-r requirements.txt (line 3)) (4.56.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers==0.9.4\n",
      "  Downloading tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.2.0->-r requirements.txt (line 3)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers==4.2.0->-r requirements.txt (line 3)) (3.7.2)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.2.0->-r requirements.txt (line 3)) (20.9)\u001b[0m\n",
      "\u001b[34mCollecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.2.0->-r requirements.txt (line 3)) (2.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas==1.1.5->-r requirements.txt (line 2)) (1.15.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.2.0->-r requirements.txt (line 3)) (3.7.4.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.2.0->-r requirements.txt (line 3)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.2.0->-r requirements.txt (line 3)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.2.0->-r requirements.txt (line 3)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.2.0->-r requirements.txt (line 3)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.2.0->-r requirements.txt (line 3)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.2.0->-r requirements.txt (line 3)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.2.0->-r requirements.txt (line 3)) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.2.0->-r requirements.txt (line 3)) (1.0.1)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, tokenizers, sacremoses, numpy, filelock, transformers, pandas\u001b[0m\n",
      "\n",
      "2021-06-24 20:33:36 Training - Training image download completed. Training in progress.\u001b[34m  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.16.4\n",
      "    Uninstalling numpy-1.16.4:\n",
      "      Successfully uninstalled numpy-1.16.4\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 0.25.0\n",
      "    Uninstalling pandas-0.25.0:\n",
      "      Successfully uninstalled pandas-0.25.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed filelock-3.0.12 numpy-1.19.5 pandas-1.1.5 regex-2021.4.4 sacremoses-0.0.45 tokenizers-0.9.4 transformers-4.2.0\n",
      "\u001b[0m\n",
      "\u001b[34m2021-06-24 20:33:33,080 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"weight_decay\": 0.01,\n",
      "        \"max_sequence_length\": 128,\n",
      "        \"train_batch_size\": 32,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"max_data_rows\": 1000,\n",
      "        \"epochs\": 2,\n",
      "        \"learning_rate\": 0.0005,\n",
      "        \"input_path\": \"s3://sagemaker-dan-rasmussen/sagemaker/amazon_review_classifier/train/input_data/small_book_reviews.json\",\n",
      "        \"valid_batch_size\": 128\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"amazon-review-model-20210624202906\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-dan-rasmussen/sagemaker/amazon_review_classifier/train/src/amazon-review-model-20210624202906/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"model\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"model.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":2,\"input_path\":\"s3://sagemaker-dan-rasmussen/sagemaker/amazon_review_classifier/train/input_data/small_book_reviews.json\",\"learning_rate\":0.0005,\"max_data_rows\":1000,\"max_sequence_length\":128,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32,\"valid_batch_size\":128,\"weight_decay\":0.01}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=model.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=model\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-dan-rasmussen/sagemaker/amazon_review_classifier/train/src/amazon-review-model-20210624202906/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":2,\"input_path\":\"s3://sagemaker-dan-rasmussen/sagemaker/amazon_review_classifier/train/input_data/small_book_reviews.json\",\"learning_rate\":0.0005,\"max_data_rows\":1000,\"max_sequence_length\":128,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32,\"valid_batch_size\":128,\"weight_decay\":0.01},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"amazon-review-model-20210624202906\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-dan-rasmussen/sagemaker/amazon_review_classifier/train/src/amazon-review-model-20210624202906/source/sourcedir.tar.gz\",\"module_name\":\"model\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"model.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"2\",\"--input_path\",\"s3://sagemaker-dan-rasmussen/sagemaker/amazon_review_classifier/train/input_data/small_book_reviews.json\",\"--learning_rate\",\"0.0005\",\"--max_data_rows\",\"1000\",\"--max_sequence_length\",\"128\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"32\",\"--valid_batch_size\",\"128\",\"--weight_decay\",\"0.01\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGHT_DECAY=0.01\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SEQUENCE_LENGTH=128\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_DATA_ROWS=1000\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0005\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT_PATH=s3://sagemaker-dan-rasmussen/sagemaker/amazon_review_classifier/train/input_data/small_book_reviews.json\u001b[0m\n",
      "\u001b[34mSM_HP_VALID_BATCH_SIZE=128\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 model.py --epochs 2 --input_path s3://sagemaker-dan-rasmussen/sagemaker/amazon_review_classifier/train/input_data/small_book_reviews.json --learning_rate 0.0005 --max_data_rows 1000 --max_sequence_length 128 --model_name distilbert-base-uncased --train_batch_size 32 --valid_batch_size 128 --weight_decay 0.01\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mData contains 1000 rows\u001b[0m\n",
      "\u001b[34m[2021-06-24 20:33:52.354 algo-1:31 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-06-24 20:33:52.355 algo-1:31 INFO hook.py:192] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-06-24 20:33:52.355 algo-1:31 INFO hook.py:237] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-06-24 20:33:52.355 algo-1:31 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-06-24 20:33:52.380 algo-1:31 INFO hook.py:382] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-06-24 20:33:52.380 algo-1:31 INFO hook.py:443] Hook is writing from the hook with pid: 31\n",
      "\u001b[0m\n",
      "\u001b[34m{'loss': 0.6798, 'learning_rate': 1e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.6711795926094055, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_auprc': 0.408, 'eval_runtime': 0.8538, 'eval_samples_per_second': 292.793, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m{'loss': 0.6868, 'learning_rate': 2e-05, 'epoch': 0.83}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.6459206342697144, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_auprc': 0.408, 'eval_runtime': 0.8475, 'eval_samples_per_second': 294.986, 'epoch': 0.83}\u001b[0m\n",
      "\u001b[34m{'loss': 0.6264, 'learning_rate': 3e-05, 'epoch': 1.25}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.5598294138908386, 'eval_precision': 0.8947368421052632, 'eval_recall': 0.5, 'eval_auprc': 0.6513684210526316, 'eval_runtime': 0.8553, 'eval_samples_per_second': 292.298, 'epoch': 1.25}\u001b[0m\n",
      "\u001b[34m{'loss': 0.4867, 'learning_rate': 4e-05, 'epoch': 1.67}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.35533010959625244, 'eval_precision': 0.8586956521739131, 'eval_recall': 0.7745098039215687, 'eval_auprc': 0.7570682011935209, 'eval_runtime': 0.8535, 'eval_samples_per_second': 292.911, 'epoch': 1.67}\u001b[0m\n",
      "\u001b[34m{'train_runtime': 24.3476, 'train_samples_per_second': 1.971, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34mSave model...\u001b[0m\n",
      "\u001b[34mModel path: /opt/ml/model/model.pth\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/442 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 442/442 [00:00<00:00, 531kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]#015Downloading:  87%|████████▋ | 201k/232k [00:00<00:00, 1.54MB/s]#015Downloading: 100%|██████████| 232k/232k [00:00<00:00, 1.30MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]#015Downloading:  37%|███▋      | 172k/466k [00:00<00:00, 1.25MB/s]#015Downloading: 100%|██████████| 466k/466k [00:00<00:00, 2.05MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]#015Downloading:   1%|          | 2.45M/268M [00:00<00:10, 24.5MB/s]#015Downloading:   2%|▏         | 4.90M/268M [00:00<00:12, 21.7MB/s]#015Downloading:   3%|▎         | 7.10M/268M [00:00<00:12, 21.0MB/s]#015Downloading:   4%|▍         | 11.0M/268M [00:00<00:09, 27.9MB/s]#015Downloading:   6%|▌         | 15.7M/268M [00:00<00:07, 33.6MB/s]#015Downloading:   8%|▊         | 21.4M/268M [00:00<00:05, 41.5MB/s]#015Downloading:  10%|█         | 27.3M/268M [00:00<00:05, 47.1MB/s]#015Downloading:  12%|█▏        | 32.1M/268M [00:00<00:05, 42.6MB/s]#015Downloading:  14%|█▍        | 37.0M/268M [00:00<00:05, 44.3MB/s]#015Downloading:  15%|█▌        | 41.5M/268M [00:01<00:05, 39.9MB/s]#015Downloading:  17%|█▋        | 46.3M/268M [00:01<00:05, 42.1MB/s]#015Downloading:  19%|█▉        | 50.6M/268M [00:01<00:05, 36.6MB/s]#015Downloading:  21%|██        | 56.6M/268M [00:01<00:04, 42.6MB/s]#015Downloading:  23%|██▎       | 62.4M/268M [00:01<00:04, 46.6MB/s]#015Downloading:  26%|██▌       | 69.3M/268M [00:01<00:03, 53.0MB/s]#015Downloading:  29%|██▊       | 76.5M/268M [00:01<00:03, 58.4MB/s]#015Downloading:  31%|███       | 82.6M/268M [00:01<00:03, 57.3MB/s]#015Downloading:  33%|███▎      | 88.4M/268M [00:01<00:03, 56.3MB/s]#015Downloading:  36%|███▌      | 95.3M/268M [00:02<00:02, 59.9MB/s]#015Downloading:  38%|███▊      | 101M/268M [00:02<00:02, 56.1MB/s] #015Downloading:  40%|███▉      | 107M/268M [00:02<00:02, 54.7MB/s]#015Downloading:  43%|████▎     | 115M/268M [00:02<00:02, 60.5MB/s]#015Downloading:  45%|████▌     | 122M/268M [00:02<00:02, 64.0MB/s]#015Downloading:  48%|████▊     | 128M/268M [00:02<00:02, 56.6MB/s]#015Downloading:  50%|█████     | 134M/268M [00:02<00:02, 56.9MB/s]#015Downloading:  52%|█████▏    | 140M/268M [00:03<00:03, 40.8MB/s]#015Downloading:  54%|█████▍    | 145M/268M [00:03<00:03, 32.7MB/s]#015Downloading:  56%|█████▌    | 149M/268M [00:03<00:03, 32.0MB/s]#015Downloading:  57%|█████▋    | 153M/268M [00:03<00:03, 29.5MB/s]#015Downloading:  59%|█████▉    | 158M/268M [00:03<00:03, 34.4MB/s]#015Downloading:  61%|██████    | 164M/268M [00:03<00:02, 40.1MB/s]#015Downloading:  64%|██████▍   | 171M/268M [00:03<00:01, 48.4MB/s]#015Downloading:  66%|██████▌   | 177M/268M [00:03<00:01, 51.7MB/s]#015Downloading:  69%|██████▉   | 185M/268M [00:04<00:01, 58.0MB/s]#015Downloading:  71%|███████▏  | 191M/268M [00:04<00:01, 59.2MB/s]#015Downloading:  74%|███████▍  | 198M/268M [00:04<00:01, 63.1MB/s]#015Downloading:  76%|███████▋  | 205M/268M [00:04<00:01, 61.7MB/s]#015Downloading:  79%|███████▉  | 211M/268M [00:04<00:01, 55.5MB/s]#015Downloading:  82%|████████▏ | 219M/268M [00:04<00:00, 60.8MB/s]#015Downloading:  84%|████████▍ | 226M/268M [00:04<00:00, 65.4MB/s]#015Downloading:  87%|████████▋ | 234M/268M [00:04<00:00, 68.5MB/s]#015Downloading:  90%|████████▉ | 241M/268M [00:05<00:00, 49.1MB/s]#015Downloading:  92%|█████████▏| 247M/268M [00:05<00:00, 51.7MB/s]#015Downloading:  94%|█████████▍| 253M/268M [00:05<00:00, 48.8MB/s]#015Downloading:  96%|█████████▋| 258M/268M [00:05<00:00, 48.6MB/s]#015Downloading:  98%|█████████▊| 263M/268M [00:05<00:00, 48.0MB/s]#015Downloading: 100%|██████████| 268M/268M [00:05<00:00, 48.2MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/48 [00:00<?, ?it/s]#015  2%|▏         | 1/48 [00:01<01:00,  1.28s/it]#015  4%|▍         | 2/48 [00:01<00:32,  1.40it/s]#015  6%|▋         | 3/48 [00:01<00:23,  1.88it/s]#015  8%|▊         | 4/48 [00:02<00:19,  2.24it/s]#015 10%|█         | 5/48 [00:02<00:17,  2.47it/s]#015 12%|█▎        | 6/48 [00:02<00:15,  2.66it/s]#015 15%|█▍        | 7/48 [00:03<00:14,  2.80it/s]#015 17%|█▋        | 8/48 [00:03<00:13,  2.91it/s]#015 19%|█▉        | 9/48 [00:03<00:12,  3.02it/s]#015 21%|██        | 10/48 [00:04<00:12,  3.09it/s]#015                                               #015#015 21%|██        | 10/48 [00:04<00:12,  3.09it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/2 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 2/2 [00:00<00:00,  5.39it/s]#033[A/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\u001b[0m\n",
      "\u001b[34m#015                                               #015\u001b[0m\n",
      "\u001b[34m#015                                             #015#033[A#015 21%|██        | 10/48 [00:04<00:12,  3.09it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 2/2 [00:00<00:00,  5.39it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015                                             #033[A#015 23%|██▎       | 11/48 [00:06<00:30,  1.21it/s]#015 25%|██▌       | 12/48 [00:06<00:24,  1.49it/s]#015 27%|██▋       | 13/48 [00:06<00:19,  1.77it/s]#015 29%|██▉       | 14/48 [00:07<00:16,  2.04it/s]#015 31%|███▏      | 15/48 [00:07<00:14,  2.28it/s]#015 33%|███▎      | 16/48 [00:07<00:12,  2.50it/s]#015 35%|███▌      | 17/48 [00:07<00:11,  2.66it/s]#015 38%|███▊      | 18/48 [00:08<00:10,  2.81it/s]#015 40%|███▉      | 19/48 [00:08<00:09,  2.90it/s]#015 42%|████▏     | 20/48 [00:08<00:09,  2.99it/s]#015                                               #015#015 42%|████▏     | 20/48 [00:08<00:09,  2.99it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/2 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 2/2 [00:00<00:00,  5.18it/s]#033[A/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\u001b[0m\n",
      "\u001b[34m#015                                               #015\u001b[0m\n",
      "\u001b[34m#015                                             #015#033[A#015 42%|████▏     | 20/48 [00:09<00:09,  2.99it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 2/2 [00:00<00:00,  5.18it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015                                             #033[A#015 44%|████▍     | 21/48 [00:10<00:22,  1.21it/s]#015 46%|████▌     | 22/48 [00:11<00:17,  1.48it/s]#015 48%|████▊     | 23/48 [00:11<00:14,  1.74it/s]#015 50%|█████     | 24/48 [00:11<00:10,  2.24it/s]#015 52%|█████▏    | 25/48 [00:12<00:09,  2.45it/s]#015 54%|█████▍    | 26/48 [00:12<00:08,  2.62it/s]#015 56%|█████▋    | 27/48 [00:12<00:07,  2.77it/s]#015 58%|█████▊    | 28/48 [00:12<00:06,  2.88it/s]#015 60%|██████    | 29/48 [00:13<00:06,  2.95it/s]#015 62%|██████▎   | 30/48 [00:13<00:06,  2.98it/s]#015                                               #015#015 62%|██████▎   | 30/48 [00:13<00:06,  2.98it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/2 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 2/2 [00:00<00:00,  5.30it/s]#033[A#015                                               #015\u001b[0m\n",
      "\u001b[34m#015                                             #015#033[A#015 62%|██████▎   | 30/48 [00:14<00:06,  2.98it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 2/2 [00:00<00:00,  5.30it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015                                             #033[A#015 65%|██████▍   | 31/48 [00:15<00:14,  1.21it/s]#015 67%|██████▋   | 32/48 [00:15<00:10,  1.47it/s]#015 69%|██████▉   | 33/48 [00:16<00:08,  1.75it/s]#015 71%|███████   | 34/48 [00:16<00:06,  2.02it/s]#015 73%|███████▎  | 35/48 [00:16<00:05,  2.25it/s]#015 75%|███████▌  | 36/48 [00:17<00:04,  2.47it/s]#015 77%|███████▋  | 37/48 [00:17<00:04,  2.66it/s]#015 79%|███████▉  | 38/48 [00:17<00:03,  2.78it/s]#015 81%|████████▏ | 39/48 [00:18<00:03,  2.88it/s]#015 83%|████████▎ | 40/48 [00:18<00:02,  2.97it/s]#015                                               #015#015 83%|████████▎ | 40/48 [00:18<00:02,  2.97it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/2 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 2/2 [00:00<00:00,  5.28it/s]#033[A#015                                               #015\u001b[0m\n",
      "\u001b[34m2021-06-24 20:34:21,330 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34m#015                                             #015#033[A#015 83%|████████▎ | 40/48 [00:19<00:02,  2.97it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 2/2 [00:00<00:00,  5.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015                                             #033[A#015 85%|████████▌ | 41/48 [00:20<00:05,  1.19it/s]#015 88%|████████▊ | 42/48 [00:20<00:04,  1.46it/s]#015 90%|████████▉ | 43/48 [00:21<00:02,  1.74it/s]#015 92%|█████████▏| 44/48 [00:21<00:01,  2.01it/s]#015 94%|█████████▍| 45/48 [00:21<00:01,  2.25it/s]#015 96%|█████████▌| 46/48 [00:22<00:00,  2.48it/s]#015 98%|█████████▊| 47/48 [00:22<00:00,  2.64it/s]#015100%|██████████| 48/48 [00:22<00:00,  3.22it/s]#015                                               #015#015100%|██████████| 48/48 [00:24<00:00,  3.22it/s]#015100%|██████████| 48/48 [00:24<00:00,  1.97it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/2 [00:00<?, ?it/s]#015100%|██████████| 2/2 [00:00<00:00,  5.28it/s]#015100%|██████████| 2/2 [00:00<00:00,  2.67it/s]\u001b[0m\n",
      "\u001b[34mINFO:__main__:Save model...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/transformers/modeling_utils.py:1760: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  input_tensor.shape[chunk_dim] == tensor_shape for input_tensor in input_tensors\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/torch/jit/__init__.py:1037: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\u001b[0m\n",
      "\u001b[34mNot within tolerance rtol=1e-05 atol=1e-05 at input[0, 1] (-0.22723621129989624 vs. -0.5716753602027893) and 1 other locations (100.00%)\n",
      "  check_tolerance, _force_outplace, True, _module_class)\u001b[0m\n",
      "\u001b[34mINFO:__main__:Model path: /opt/ml/model/model.pth\n",
      "\u001b[0m\n",
      "\n",
      "2021-06-24 20:34:36 Uploading - Uploading generated training model\n",
      "2021-06-24 20:39:38 Completed - Training job completed\n",
      "Training seconds: 477\n",
      "Billable seconds: 477\n"
     ]
    }
   ],
   "source": [
    "# Create SageMaker estimator and laucn training job.\n",
    "pytorch_estimator = PyTorch(\n",
    "    entry_point='model.py', # The name of our model script.\n",
    "    source_dir='./src',\n",
    "    #instance_type='ml.p2.xlarge', # Instnace with GPUs.\n",
    "    instance_type='ml.g4dn.xlarge', # Instnace with GPUs.\n",
    "    instance_count=1,\n",
    "    framework_version='1.5.0', # PyTorch version.\n",
    "    py_version='py3',\n",
    "    hyperparameters=hyperparameters, # Passed as command-line args to entry_point.\n",
    "    code_location=code_path, # Where our source_dir gets stored in S3.\n",
    "    output_path=output_path, # Where our model outputs get stored in S3.\n",
    "    role=role, # Role with SageMaker access.\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "pytorch_estimator.fit(inputs=None, job_name=f\"amazon-review-model-{datetime.now().strftime('%Y%m%d%H%M%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aef1eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "# Deploy my estimator to a SageMaker Endpoint and get a Predictor\n",
    "predictor = pytorch_estimator.deploy(instance_type='ml.m5.xlarge',\n",
    "                                     initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8dd95bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[-0.9475367665290833, 1.2128119468688965]]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = JSONDeserializer()\n",
    "\n",
    "text = \"A Must!!! I have found this book to be very helpful in my practice and defer to the wisdom within quite often.  I think Fagbemijo has put together a wonderful piece of work that is not only based in tradition, but allows for adaptation in terms of bringing things back into balance.  While there are other books out there that provide an odu verse or two, &#34;An Exploration...&#34; gives us multiple to choose from that may fit the client's problem more explicitly.  Moreover, we aren't presented with prescriptions for obscure African herbs and offerings of 20,000 cowries. Who has time to figure out the conversion rate on that?  I initially thought the absence of remedies from the book was a bit odd, but in truth it allows for the diviner and their Ifa to determine what might fit best the problem in this day and age.  What an inspired move.  A must have.  I highly recommend!\"\n",
    "\n",
    "response = predictor.predict([text], initial_args={'ContentType': 'application/json', 'Accept': 'application/json'})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49896f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up resources\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0641c75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"A Must!!! I have found this book to be very helpful in my practice and defer to the wisdom within quite often.  I think Fagbemijo has put together a wonderful piece of work that is not only based in tradition, but allows for adaptation in terms of bringing things back into balance.  While there are other books out there that provide an odu verse or two, &#34;An Exploration...&#34; gives us multiple to choose from that may fit the client\\'s problem more explicitly.  Moreover, we aren\\'t presented with prescriptions for obscure African herbs and offerings of 20,000 cowries. Who has time to figure out the conversion rate on that?  I initially thought the absence of remedies from the book was a bit odd, but in truth it allows for the diviner and their Ifa to determine what might fit best the problem in this day and age.  What an inspired move.  A must have.  I highly recommend!\"]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '27cb190f-f518-4299-a791-7822587a457a',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '27cb190f-f518-4299-a791-7822587a457a',\n",
       "   'x-amzn-invoked-production-variant': 'AllTraffic',\n",
       "   'date': 'Thu, 24 Jun 2021 21:16:24 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '45'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ContentType': 'application/json',\n",
       " 'InvokedProductionVariant': 'AllTraffic',\n",
       " 'Body': <botocore.response.StreamingBody at 0x7f68882f4cc0>}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[[-1.0770901441574097, 1.2103785276412964]]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "from IPython.display import display\n",
    "\n",
    "runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "text = \"A Must!!! I have found this book to be very helpful in my practice and defer to the wisdom within quite often.  I think Fagbemijo has put together a wonderful piece of work that is not only based in tradition, but allows for adaptation in terms of bringing things back into balance.  While there are other books out there that provide an odu verse or two, &#34;An Exploration...&#34; gives us multiple to choose from that may fit the client's problem more explicitly.  Moreover, we aren't presented with prescriptions for obscure African herbs and offerings of 20,000 cowries. Who has time to figure out the conversion rate on that?  I initially thought the absence of remedies from the book was a bit odd, but in truth it allows for the diviner and their Ifa to determine what might fit best the problem in this day and age.  What an inspired move.  A must have.  I highly recommend!\"\n",
    "body = json.dumps([text])\n",
    "display(body)\n",
    "\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName='pytorch-training-2021-06-24-20-52-43-616',\n",
    "    ContentType='application/json',\n",
    "    Accept='application/json',\n",
    "    Body=body\n",
    ")\n",
    "display(response)\n",
    "result = json.loads(response['Body'].read().decode()) \n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fa0349",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87688c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f54de136",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-91a93b241329>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/sagemaker_train_demo/src/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from src import model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6de812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.get_tokenizer(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93fbbe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.encode_sequences([\"hello world\", \"hello world\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10bf011b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/modeling_utils.py:1760: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  input_tensor.shape[chunk_dim] == tensor_shape for input_tensor in input_tensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path: ./model.pth\n"
     ]
    }
   ],
   "source": [
    "dummy_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    torchscript=True\n",
    ")\n",
    "model.save_model(dummy_model, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b368797d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_fn\n"
     ]
    }
   ],
   "source": [
    "dummy_model = model.model_fn('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bbdc6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_fn\n",
      "request_content_type: application/json\n",
      "b'[\"hello world\", \"hello world\"]'\n",
      "['hello world', 'hello world']\n"
     ]
    }
   ],
   "source": [
    "data = json.dumps(['hello world', 'hello world']).encode('utf-8')\n",
    "input_data = model.input_fn(data, 'application/json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a650800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_fn\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0137, 0.1074],\n",
       "         [0.0137, 0.1074]]),)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_fn(input_data, dummy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37087527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2585e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
